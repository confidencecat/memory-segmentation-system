# AI 대화형 기억 시스템

## 1. 제작 이유

본 프로젝트는 인공지능과의 장기 대화에서 발생하는 맥락 손실, 반복 질문, 과거 지시사항의 망각 등 기존 챗봇의 한계를 극복하기 위해 시작되었다.  
일반적인 챗봇은 입력 토큰 제한으로 인해 대화의 흐름이 길어질수록 과거 맥락을 잃거나, 요약 과정에서 중요한 정보가 누락되는 문제가 있다.  
이 시스템은 대화의 주제별로 기억을 분리하여 저장하고, 필요 시 과거의 원본 대화를 유연하게 검색·활용하는 새로운 기억 구조를 구현하고자 한다.

## 2. 시스템 구조 및 작동 흐름

시스템은 다음과 같은 주요 컴포넌트로 구성된다.

- **MainAI**: 사용자와 직접 대화하며, 입력 분석, 응답 생성, 전체 대화 흐름을 제어한다.
- **AuxiliaryAI**: 기억의 저장, 요약, 분리, 주제 변경 감지 등 메모리 관리의 핵심 역할을 수행한다.
- **LoadAI**: 분리된 장기 기억(Separated Memory)에서 관련 정보를 병렬적으로 검색한다.
- **MemoryManager/DataManager**: JSON 기반의 데이터 입출력, 파일 시스템 관리, 환경 변수 관리 등을 담당한다.

### 작동 흐름

1. **사용자 입력**: 사용자의 질문을 입력받는다.
2. **기억 필요성 판단**: 입력이 과거 대화의 기억을 필요로 하는지 LLM을 통해 판단한다.
3. **기억 검색**: 필요하다면, LoadAI가 주제별 요약(SEP_MEMORY, SUM_MEMORY)을 병렬로 검사하여 관련 기억을 찾는다.
4. **응답 생성**: 관련 기억이 있다면 이를 바탕으로, 없다면 단순히 현재 입력만으로 응답을 생성한다.
5. **기억 관리**: AuxiliaryAI가 현재 대화와 요약을 비교하여 주제 변경 여부를 판단한다.
    - 주제가 변경되었다면, 기존 대화와 요약을 분리된 기억(SEP_MEMORY)에 저장하고, 버퍼와 요약을 초기화한다.
    - 주제가 연속된다면, 요약을 업데이트하고 대화 내용을 버퍼에 추가한다.
6. **전체 대화 기록**: 모든 대화는 all_memory.json에 순차적으로 저장된다.

## 3. 데이터 구조 및 저장 방식

- **all_memory.json**: 전체 대화의 원본을 순차적으로 저장한다.
- **sep_memory.json**: 주제별로 분리된 장기 기억을 저장하며, 각 항목은 요약(SUMMARIZATION), 버퍼 인덱스(NUM), 전체 대화 인덱스(LOAD)를 포함한다.
- **buf_memory.json**: 현재 주제의 단기 대화 기록(버퍼)이다.
- **sum_memory.json**: 현재 주제의 대화 요약이다.
- **env_memory.json**: BUF_COUNTER(주제 수), CON_COUNTER(전체 대화 수) 등 환경 정보를 저장한다.

## 4. 판단 및 검색 방식

### 판단 로직

- **기억 필요성(f_need)**: 사용자의 입력이 과거 기억을 필요로 하는지 LLM이 판단한다. (예: "저번에 얘기한 내용 알려줘" → True)
- **주제 변경(f_change)**: 기존 요약과 새로운 대화를 비교하여 주제가 크게 바뀌었는지 LLM이 판단한다. (예: "사과 이야기" → "포도 이야기" = True)
- **연관성(f_rel)**: 사용자의 질문과 각 기억 요약의 연관성을 LLM이 병렬로 판단한다. (True/False)

### 검색 및 응답

- LoadAI는 SEP_MEMORY와 SUM_MEMORY의 요약을 모두 검사한다.
- 관련성이 True로 판단된 기억의 인덱스를 수집하여, 해당 구간의 원본 대화를 all_memory.json에서 추출한다.
- MainAI는 이 기억 데이터를 history로 받아, 맥락에 맞는 응답을 생성한다.

## 5. 주요 코드 흐름 및 컴포넌트 설명

### MainAI

- 사용자 입력을 받아 기억 필요성 판단 → 기억 검색 → 응답 생성 → 대화 저장 및 기억 관리의 전체 흐름을 담당한다.
- 응답 생성 시, 관련 기억이 있으면 history로 활용하여 맥락 있는 답변을 생성한다.

### AuxiliaryAI

- 대화의 요약 생성, 기존 요약과의 통합, 주제 변경 감지, 기억 분리 및 저장, 환경 변수 관리 등 기억의 흐름을 총괄한다.
- 주제 변경이 감지되면 SEP_MEMORY에 저장하고, 버퍼와 요약을 초기화한다.

### LoadAI

- SEP_MEMORY와 SUM_MEMORY의 요약을 병렬로 LLM에 전달하여, 사용자의 질문과 연관된 기억을 빠르게 찾는다.
- 관련 인덱스의 원본 대화를 all_memory.json에서 추출하여 MainAI에 전달한다.

### MemoryManager/DataManager

- 모든 메모리 파일의 존재를 보장하고, JSON 데이터의 입출력, 환경 변수의 관리, 대화 기록의 문자열 변환 등을 담당한다.

## 6. 수식 및 논리적 표현

- `U`: 사용자 입력
- `R`: AI 응답
- `M_sep`: 분리된 기억 집합
- `M_buf`: 현재 대화 버퍼
- `S(m)`: 기억 m의 요약
- `f_need(U)`: 기억 필요성 판단 함수
- `f_rel(U, S(m))`: 입력과 요약의 연관성 판단 함수
- `f_change(S(M_buf), U)`: 주제 변경 감지 함수
- `G(U, M)`: 입력 U와 기억 M을 기반으로 응답 생성 함수

### 프로세스

1. **기억 검색**  
   `M_rel = {m ∈ M_sep ∪ {M_buf} | f_need(U) ∧ f_rel(U, S(m))}`

2. **응답 생성**  
   `R = G(U, M_rel)`

3. **기억 업데이트**  
   - If `f_change(S(M_buf), U)` is True:  
     `M_sep_new = M_sep ∪ {M_buf}`  
     `M_buf_new = {(U, R)}`
   - Else:  
     `M_buf_new = M_buf ∪ {(U, R)}`

## 7. 특징 및 장점

- **주제별 기억 분리**: 대화의 흐름이 바뀔 때마다 기억을 분리하여 저장함으로써, 맥락 손실을 최소화한다.
- **LLM 기반 판단**: 모든 판단(기억 필요성, 주제 변경, 연관성 등)을 LLM이 수행하여, 단순 유사도 기반 검색보다 유연하고 정확하다.
- **병렬 검색**: API 키를 여러 개 사용하면 기억 검색 속도가 빨라진다.
- **원본 대화 보존**: 요약뿐 아니라 원본 대화도 함께 저장하여, 정보 손실을 방지한다.

## 8. 실행 및 확장

- 환경 변수(.env)에 API 키를 등록해야 한다.
- memory/ 폴더에 각종 json 파일이 자동 생성된다.
- API 키를 여러 개 등록하면 LoadAI의 병렬 검색 성능이 향상된다.
- 각 판단에 사용되는 few-shot 예제는 config.py에서 관리한다.

---

**참고:**  
본 README는 실제 코드(main.py)와 시스템 구조에 기반하여 작성하였다.  
코드와 설명이 상충되는 부분이 있다면 알려주면 즉시 수정 및 안내하겠다.
